{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison for Amharic NER\n",
    "This notebook compares multiple NER models (XLM-RoBERTa, mBERT, DistilBERT, etc.) on Amharic Telegram e-commerce data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    ")\n",
    "import evaluate\n",
    "from src.utils.ner_data_utils import parse_conll, build_label_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load and prepare data\n",
    "conll_path = '../data/raw/labeled_cnll_manual.txt'  # Adjust if needed\n",
    "sentences, ner_tags = parse_conll(conll_path)\n",
    "label2id, id2label = build_label_maps(ner_tags)\n",
    "data = pd.DataFrame({'tokens': sentences, 'ner_tags': ner_tags})\n",
    "dataset = Dataset.from_pandas(data)\n",
    "if len(dataset) > 1:\n",
    "    train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test['train']\n",
    "    eval_dataset = train_test['test']\n",
    "else:\n",
    "    train_dataset = dataset\n",
    "    eval_dataset = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Models to compare\n",
    "model_names = [\n",
    "    'xlm-roberta-base',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'Davlan/bert-tiny-amharic-ner',\n",
    "    'Davlan/afro-xlmr-mini',\n",
    "    'distilbert-base-multilingual-cased'\n",
    "]\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tokenization and label alignment\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_idx]] if label[word_idx].startswith('I-') else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training & evaluation loop\n",
    "def train_and_evaluate(model_name, train_dataset, eval_dataset, label2id, id2label):\n",
    "    global tokenizer\n",
    "    print(f'\\n==== Training {model_name} ====')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    tokenized_eval = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{model_name.replace('/', '_')}',\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy='no',\n",
    "        logging_dir=f'./logs/{model_name.replace('/', '_')}',\n",
    "        fp16=True if torch.cuda.is_available() else False,\n",
    "        report_to='none'\n",
    "    )\n",
    "    metric = evaluate.load('seqeval')\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        true_predictions = [\n",
    "            [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    elapsed = time.time() - start_time\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'f1': metrics.get('eval_overall_f1', 0),\n",
    "        'precision': metrics.get('eval_overall_precision', 0),\n",
    "        'recall': metrics.get('eval_overall_recall', 0),\n",
    "        'accuracy': metrics.get('eval_overall_accuracy', 0),\n",
    "        'train_time_sec': elapsed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run model comparison\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        res = train_and_evaluate(model_name, train_dataset, eval_dataset, label2id, id2label)\n",
    "        results.append(res)\n",
    "    except Exception as e:\n",
    "        print(f'Error with {model_name}: {e}')\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('./results/model_comparison.csv', index=False)\n",
    "print('Model comparison results:')\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
