{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vendor Analytics Engine\n",
    "This notebook combines NER-extracted entities with Telegram post metadata to create a vendor scorecard with business insights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from src.utils.ner_data_utils import build_label_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Telegram messages with metadata\n",
    "jsonl_path = '../data/telegram_messages.jsonl'\n",
    "messages = []\n",
    "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        messages.append(json.loads(line))\n",
    "df = pd.DataFrame(messages)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load NER model\n",
    "MODEL_DIR = 'notebooks/amharicnermodel'  # Adjust as needed\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Helper: Run NER model to extract entities from text\n",
    "def extract_entities(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    preds = torch.argmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    # Build label maps (assume model.config.id2label exists)\n",
    "    id2label = model.config.id2label if hasattr(model.config, 'id2label') else {i: str(i) for i in range(logits.shape[-1])}\n",
    "    entities = []\n",
    "    current = None\n",
    "    for token, pred in zip(tokens, preds):\n",
    "        label = id2label[str(pred)] if isinstance(id2label, dict) else id2label[pred]\n",
    "        if label.startswith('B-') or (label != 'O' and (current is None or label != current['label'])):\n",
    "            if current: entities.append(current)\n",
    "            current = {'label': label[2:] if label.startswith('B-') else label, 'tokens': [token]}\n",
    "        elif label.startswith('I-') and current and label[2:] == current['label']:\n",
    "            current['tokens'].append(token)\n",
    "        else:\n",
    "            if current: entities.append(current); current = None\n",
    "    if current: entities.append(current)\n",
    "    # Join tokens for each entity\n",
    "    for ent in entities:\n",
    "        ent['text'] = tokenizer.convert_tokens_to_string(ent['tokens'])\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract entities and enrich dataframe\n",
    "def extract_price(entities):\n",
    "    for ent in entities:\n",
    "        if ent['label'].lower() == 'price':\n",
    "            # Try to extract numeric value\n",
    "            try:\n",
    "                return float(''.join([c for c in ent['text'] if c.isdigit() or c == '.']))\n",
    "            except:\n",
    "                return None\n",
    "    return None\n",
    "df['entities'] = df['text'].apply(extract_entities)\n",
    "df['price'] = df['entities'].apply(extract_price)\n",
    "print(df[['channel', 'text', 'price']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate metrics per vendor/channel\n",
    "def posts_per_week(timestamps):\n",
    "    dates = pd.to_datetime(timestamps)\n",
    "    if len(dates) < 2:\n",
    "        return 0\n",
    "    weeks = (dates.max() - dates.min()).days / 7.0\n",
    "    return len(dates) / weeks if weeks > 0 else len(dates)\n",
    "scorecard = []\n",
    "for channel, group in df.groupby('channel'):\n",
    "    avg_views = group['views'].mean() if 'views' in group else np.nan\n",
    "    freq = posts_per_week(group['timestamp'])\n",
    "    avg_price = group['price'].mean() if group['price'].notnull().any() else np.nan\n",
    "    top_post = group.loc[group['views'].idxmax()] if 'views' in group and group['views'].notnull().any() else None\n",
    "    lending_score = (avg_views or 0) * 0.5 + freq * 0.5\n",
    "    scorecard.append({\n",
    "        'Vendor': channel,\n",
    "        'Avg. Views/Post': avg_views,\n",
    "        'Posts/Week': freq,\n",
    "        'Avg. Price (ETB)': avg_price,\n",
    "        'Lending Score': lending_score,\n",
    "        'Top Post': top_post['text'] if top_post is not None else '',\n",
    "        'Top Post Views': top_post['views'] if top_post is not None else '',\n",
    "        'Top Post Price': top_post['price'] if top_post is not None else ''\n",
    "    })\n",
    "scorecard_df = pd.DataFrame(scorecard)\n",
    "print(scorecard_df[['Vendor', 'Avg. Views/Post', 'Posts/Week', 'Avg. Price (ETB)', 'Lending Score']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
